• Table of Contents
Computer Security: Art and Science
By Matt Bishop
Publisher: Addison Wesley
Pub Date: November 29, 2002
ISBN: 0-201-44099-7
Pages: 1136
"This is an excellent text that should be read by every computer security professional and student."
—Dick Kemmerer, University of California, Santa Barbara.
"This is the most complete book on information security theory, technology, and practice that I have encountered
anywhere!"
—Marvin Schaefer, Former Chief Scientist, National Computer Security Center, NSA
This highly anticipated book fully introduces the theory and practice of computer security. It is both a comprehensive
text, explaining the most fundamental and pervasive aspects of the field, and a detailed reference filled with valuable
information for even the most seasoned practitioner. In this one extraordinary volume the author incorporates
concepts from computer systems, networks, human factors, and cryptography. In doing so, he effectively
demonstrates that computer security is an art as well as a science.
Computer Security: Art and Science includes detailed discussions on:
The nature and challenges of computer security
The relationship between policy and security
The role and application of cryptography
The mechanisms used to implement policies
Methodologies and technologies for assurance
Vulnerability analysis and intrusion detection
Computer Security discusses different policy models, and presents mechanisms that can be used to enforce these
policies. It concludes with examples that show how to apply the principles discussed in earlier sections, beginning with
networks and moving on to systems, users, and programs.
This important work is essential for anyone who needs to understand, implement, or maintain a secure network or
computer system.
•
Table of
Contents
Computer Security: Art and Science
By Matt Bishop
Publisher: Addison Wesley
Pub Date: November 29, 2002
ISBN: 0-201-44099-7
Pages: 1136
Copyright
Preface
Goals
Philosophy
Organization
Roadmap
Special Acknowledgment
Acknowledgments
Part 1. Introduction
Chapter 1. An Overview of Computer Security
Section 1.1. The Basic Components
Section 1.2. Threats
Section 1.3. Policy and Mechanism
Section 1.4. Assumptions and Trust
Section 1.5. Assurance
Section 1.6. Operational Issues
Section 1.7. Human Issues
Section 1.8. Tying It All Together
Section 1.9. Summary
Section 1.10. Research Issues
Section 1.11. Further Reading
Section 1.12. Exercises
Part 2. Foundations
Chapter 2. Access Control Matrix
Section 2.1. Protection State
Section 2.2. Access Control Matrix Model
Section 2.3. Protection State Transitions
Section 2.4. Copying, Owning, and the Attenuation of Privilege
Section 2.5. Summary
Section 2.6. Research Issues
Section 2.7. Further Reading
Section 2.8. Exercises
Chapter 3. Foundational Results
Section 3.1. The General Question
Section 3.2. Basic Results
Section 3.3. The Take-Grant Protection Model
Section 3.4. Closing the Gap
Section 3.5. Expressive Power and the Models
Section 3.6. Summary
Section 3.7. Research Issues
Section 3.8. Further Reading
Section 3.9. Exercises
Part 3. Policy
Chapter 4. Security Policies
Section 4.1. Security Policies
Section 4.2. Types of Security Policies
Section 4.3. The Role of Trust
Section 4.4. Types of Access Control
Section 4.5. Policy Languages
Section 4.6. Example: Academic Computer Security Policy
Section 4.7. Security and Precision
Section 4.8. Summary
Section 4.9. Research Issues
Section 4.10. Further Reading
Section 4.11. Exercises
Chapter 5. Confidentiality Policies
Section 5.1. Goals of Confidentiality Policies
Section 5.2. The Bell-LaPadula Model
Section 5.3. Tranquility
Section 5.4. The Controversy over the Bell-LaPadula Model
Section 5.5. Summary
Section 5.6. Research Issues
Section 5.7. Further Reading
Section 5.8. Exercises
Chapter 6. Integrity Policies
Section 6.1. Goals
Section 6.2. Biba Integrity Model
Section 6.3. Lipner's Integrity Matrix Model
Section 6.4. Clark-Wilson Integrity Model
Section 6.5. Summary
Section 6.6. Research Issues
Section 6.7. Further Reading
Section 6.8. Exercises
Chapter 7. Hybrid Policies
Section 7.1. Chinese Wall Model
Section 7.2. Clinical Information Systems Security Policy
Section 7.3. Originator Controlled Access Control
Section 7.4. Role-Based Access Control
Section 7.5. Summary
Section 7.6. Research Issues
Section 7.7. Further Reading
Section 7.8. Exercises
Chapter 8. Noninterference and Policy Composition
Section 8.1. The Problem
Section 8.2. Deterministic Noninterference
Section 8.3. Nondeducibility
Section 8.4. Generalized Noninterference
Section 8.5. Restrictiveness
Section 8.6. Summary
Section 8.7. Research Issues
Section 8.8. Further Reading
Section 8.9. Exercises
Part 4. Implementation I: Cryptography
Chapter 9. Basic Cryptography
Section 9.1. What Is Cryptography?
Section 9.2. Classical Cryptosystems
Section 9.3. Public Key Cryptography
Section 9.4. Cryptographic Checksums
Section 9.5. Summary
Section 9.6. Research Issues
Section 9.7. Further Reading
Section 9.8. Exercises
Chapter 10. Key Management
Section 10.1. Session and Interchange Keys
Section 10.2. Key Exchange
Section 10.3. Key Generation
Section 10.4. Cryptographic Key Infrastructures
Section 10.5. Storing and Revoking Keys
Section 10.6. Digital Signatures
Section 10.7. Summary
Section 10.8. Research Issues
Section 10.9. Further Reading
Section 10.10. Exercises
Chapter 11. Cipher Techniques
Section 11.1. Problems
Section 11.2. Stream and Block Ciphers
Section 11.3. Networks and Cryptography
Section 11.4. Example Protocols
Section 11.5. Summary
Section 11.6. Research Issues
Section 11.7. Further Reading
Section 11.8. Exercises
Chapter 12. Authentication
Section 12.1. Authentication Basics
Section 12.2. Passwords
Section 12.3. Challenge-Response
Section 12.4. Biometrics
Section 12.5. Location
Section 12.6. Multiple Methods
Section 12.7. Summary
Section 12.8. Research Issues
Section 12.9. Further Reading
Section 12.10. Exercises
Part 5. Implementation II: Systems
Chapter 13. Design Principles
Section 13.1. Overview
Section 13.2. Design Principles
Section 13.3. Summary
Section 13.4. Research Issues
Section 13.5. Further Reading
Section 13.6. Exercises
Chapter 14. Representing Identity
Section 14.1. What Is Identity?
Section 14.2. Files and Objects
Section 14.3. Users
Section 14.4. Groups and Roles
Section 14.5. Naming and Certificates
Section 14.6. Identity on the Web
Section 14.7. Summary
Section 14.8. Research Issues
Section 14.9. Further Reading
Section 14.10. Exercises
Chapter 15. Access Control Mechanisms
Section 15.1. Access Control Lists
Section 15.2. Capabilities
Section 15.3. Locks and Keys
Section 15.4. Ring-Based Access Control
Section 15.5. Propagated Access Control Lists
Section 15.6. Summary
Section 15.7. Research Issues
Section 15.8. Further Reading
Section 15.9. Exercises
Chapter 16. Information Flow
Section 16.1. Basics and Background
Section 16.2. Nonlattice Information Flow Policies
Section 16.3. Compiler-Based Mechanisms
Section 16.3. Compiler-Based Mechanisms
Section 16.4. Execution-Based Mechanisms
Section 16.5. Example Information Flow Controls
Section 16.6. Summary
Section 16.7. Research Issues
Section 16.8. Further Reading
Section 16.9. Exercises
Chapter 17. Confinement Problem
Section 17.1. The Confinement Problem
Section 17.2. Isolation
Section 17.3. Covert Channels
Section 17.4. Summary
Section 17.5. Research Issues
Section 17.6. Further Reading
Section 17.7. Exercises
Part 6. Assurance
Chapter 18. Introduction to Assurance
Section 18.1. Assurance and Trust
Section 18.2. Building Secure and Trusted Systems
Section 18.3. Summary
Section 18.4. Research Issues
Section 18.5. Further Reading
Section 18.6. Exercises
Chapter 19. Building Systems with Assurance
Section 19.1. Assurance in Requirements Definition and Analysis
Section 19.2. Assurance During System and Software Design
Section 19.3. Assurance in Implementation and Integration
Section 19.4. Assurance During Operation and Maintenance
Section 19.5. Summary
Section 19.6. Research Issues
Section 19.7. Further Reading
Section 19.8. Exercises
Chapter 20. Formal Methods
Section 20.1. Formal Verification Techniques
Section 20.2. Formal Specification
Section 20.3. Early Formal Verification Techniques
Section 20.4. Current Verification Systems
Section 20.5. Summary
Section 20.6. Research Issues
Section 20.7. Further Reading
Section 20.8. Exercises
Chapter 21. Evaluating Systems
Section 21.1. Goals of Formal Evaluation
Section 21.2. TCSEC: 1983–1999
Section 21.3. International Efforts and the ITSEC: 1991–2001
Section 21.4. Commercial International Security Requirements: 1991
Section 21.5. Other Commercial Efforts: Early 1990s
Section 21.6. The Federal Criteria: 1992
Section 21.7. FIPS 140: 1994–Present
Section 21.8. The Common Criteria: 1998–Present
Section 21.9. SSE-CMM: 1997–Present
Section 21.10. Summary
Section 21.11. Research Issues
Section 21.12. Further Reading
Section 21.13. Exercises
Part 7. Special Topics
Chapter 22. Malicious Logic
Section 22.1. Introduction
Section 22.2. Trojan Horses
Section 22.3. Computer Viruses
Section 22.4. Computer Worms
Section 22.5. Other Forms of Malicious Logic
Section 22.6. Theory of Malicious Logic
Section 22.7. Defenses
Section 22.8. Summary
Section 22.9. Research Issues
Section 22.10. Further Reading
Section 22.11. Exercises
Chapter 23. Vulnerability Analysis
Section 23.1. Introduction
Section 23.2. Penetration Studies
Section 23.3. Vulnerability Classification
Section 23.4. Frameworks
Section 23.5. Gupta and Gligor's Theory of Penetration Analysis
Section 23.6. Summary
Section 23.7. Research Issues
Section 23.8. Further Reading
Section 23.9. Exercises
Chapter 24. Auditing
Section 24.1. Definitions
Section 24.2. Anatomy of an Auditing System
Section 24.3. Designing an Auditing System
Section 24.4. A Posteriori Design
Section 24.5. Auditing Mechanisms
Section 24.6. Examples: Auditing File Systems
Section 24.7. Audit Browsing
Section 24.8. Summary
Section 24.9. Research Issues
Section 24.10. Further Reading
Section 24.11. Exercises
Chapter 25. Intrusion Detection
Section 25.1. Principles
Section 25.2. Basic Intrusion Detection
Section 25.3. Models
Section 25.4. Architecture
Section 25.5. Organization of Intrusion Detection Systems
Section 25.6. Intrusion Response
Section 25.7. Summary
Section 25.8. Research Issues
Section 25.9. Further Reading
Section 25.10. Exercises
Part 8. Practicum
Chapter 26. Network Security
Section 26.1. Introduction
Section 26.2. Policy Development
Section 26.3. Network Organization
Section 26.4. Availability and Network Flooding
Section 26.5. Anticipating Attacks
Section 26.6. Summary
Section 26.7. Research Issues
Section 26.8. Further Reading
Section 26.9. Exercises
Chapter 27. System Security
Section 27.1. Introduction
Section 27.2. Policy
Section 27.3. Networks
Section 27.4. Users
Section 27.5. Authentication
Section 27.6. Processes
Section 27.7. Files
Section 27.8. Retrospective
Section 27.9. Summary
Section 27.10. Research Issues
Section 27.11. Further Reading
Section 27.12. Exercises
Chapter 28. User Security
Section 28.1. Policy
Section 28.2. Access
Section 28.3. Files and Devices
Section 28.4. Processes
Section 28.5. Electronic Communications
Section 28.6. Summary
Section 28.7. Research Issues
Section 28.8. Further Reading
Section 28.9. Exercises
Chapter 29. Program Security
Section 29.1. Introduction
Section 29.2. Requirements and Policy
Section 29.3. Design
Section 29.4. Refinement and Implementation
Section 29.5. Common Security-Related Programming Problems
Section 29.6. Testing, Maintenance, and Operation
Section 29.7. Distribution
Section 29.8. Conclusion
Section 29.9. Summary
Section 29.10. Research Issues
Section 29.11. Further Reading
Section 29.12. Exercises
Part 9. End Matter
Chapter 30. Lattices
Section 30.1. Basics
Section 30.2. Lattices
Section 30.3. Exercises
Chapter 31. The Extended Euclidean Algorithm
Section 31.1. The Euclidean Algorithm
Section 31.2. The Extended Euclidean Algorithm
Section 31.3. Solving ax mod n = 1
Section 31.4. Solving ax mod n = b
Section 31.5. Exercises
Chapter 32. Entropy and Uncertainty
Section 32.1. Conditional and Joint Probability
Section 32.2. Entropy and Uncertainty
Section 32.3. Joint and Conditional Entropy
Section 32.4. Exercises
Chapter 33. Virtual Machines
Section 33.1. Virtual Machine Structure
Section 33.2. Virtual Machine Monitor
Section 33.3. Exercises
Chapter 34. Symbolic Logic
Section 34.1. Propositional Logic
Section 34.2. Predicate Logic
Section 34.3. Temporal Logic Systems
Section 34.4. Exercises
Section 34.4. Exercises
Chapter 35. Example Academic Security Policy
Section 35.1. University of California E-mail Policy
Section 35.2. The Acceptable Use Policy for the University of California, Davis
Bibliography
Top
Copyright
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as
trademarks. Where those designations appear in this book, and Addison-Wesley was aware of a trademark claim,
the designations have been printed with initial capital letters or in all capitals.
The author and publisher have taken care in the preparation of this book, but make no expressed or implied
warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or
consequential damages in connection with or arising out of the use of the information or programs contained
herein.
The publisher offers discounts on this book when ordered in quantity for bulk purchases and special sales. For more
information, please contact:
U.S. Corporate and Government Sales
(800) 382-3419
corpsales@pearsontechgroup.com
For sales outside of the U.S., please contact:
International Sales
(317) 581-3793
international@pearsontechgroup.com
Visit Addison-Wesley on the Web: www.awprofessional.com
Library of Congress Cataloging-in-Publication Data
Bishop, Matt.
Computer security : art and science / Matt Bishop.
p. cm.
Includes bibliographical references and index.
ISBN 0-201-44099-7 (alk. paper)
1. Computer security. I. Title.
QA76.9.A25 B56 2002
005.8—dc21 2002026219
Copyright © 2003 by Pearson Education, Inc.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in
any form, or by any means, electronic, mechanical, photocopying, recording, or otherwise, without the prior
consent of the publisher. Printed in the United States of America. Published simultaneously in Canada.
Chapters 18–21 and 34, Copyright 2003 by Elisabeth C. Sullivan. Published by Pearson Education, Inc. with
permission.
For information on obtaining permission for use of material from this work, please submit a written request to:
Pearson Education, Inc.
Rights and Contracts Department
75 Arlington Street, Suite 300
Boston, MA 02116
Fax: (617) 848-7047
Text printed on recycled paper
1 2 3 4 5 6 7 8 9 10—CRW—0605040302
First printing, November 2002
Dedication
To my dear Holly; our children Heidi, Steven, David, and Caroline; our grandson Skyler; our son-in-law Mike; and
our friends Seaview, Tinker Belle, Stripe, Baby Windsor, Fuzzy, Scout, Fur, Puff, and the rest of the menagerie.
Top
Preface
HORTENSIO: Madam, before you touch the instrument
To learn the order of my fingering,
I must begin with rudiments of art
To teach you gamouth in a briefer sort,
More pleasant, pithy and effectual,
Than hath been taught by any of my trade;
And there it is in writing, fairly drawn.
—The Taming of the Shrew, III, i, 62–68.
On September 11, 2001, terrorists seized control of four airplanes. Three were flown into buildings, and a fourth
crashed, with catastrophic loss of life. In the aftermath, the security and reliability of many aspects of society drew
renewed scrutiny. One of these aspects was the widespread use of computers and their interconnecting networks.
The issue is not new. In 1988, approximately 5,000 computers throughout the Internet were rendered unusable
within 4 hours by a program called a worm [432].[1] While the spread, and the effects, of this program alarmed
computer scientists, most people were not worried because the worm did not affect their lives or their ability to do
their jobs. In 1993, more users of computer systems were alerted to such dangers when a set of programs called
sniffers were placed on many computers run by network service providers and recorded login names and
passwords [374].
[1] Section 22.4 discusses computer worms.
After an attack on Tsutomu Shimomura's computer system, and the fascinating way Shimomura followed the
attacker's trail, which led to his arrest [914], the public's interest and apprehension were finally aroused.
Computers were now vulnerable. Their once reassuring protections were now viewed as flimsy.
Several films explored these concerns. Movies such as War Games and Hackers provided images of people who
can, at will, wander throughout computers and networks, maliciously or frivolously corrupting or destroying
information it may have taken millions of dollars to amass. (Reality intruded on Hackers when the World Wide Web
page set up by MGM/United Artists was quickly altered to present an irreverent commentary on the movie and to
suggest that viewers see The Net instead. Paramount Pictures denied doing this [448].) Another film, Sneakers,
presented a picture of those who test the security of computer (and other) systems for their owners and for the
government.
Top
Goals
This book has three goals. The first is to show the importance of theory to practice and of practice to theory. All too
often, practitioners regard theory as irrelevant and theoreticians think of practice as trivial. In reality, theory and
practice are symbiotic. For example, the theory of covert channels, in which the goal is to limit the ability of
processes to communicate through shared resources, provides a mechanism for evaluating the effectiveness of
mechanisms that confine processes, such as sandboxes and firewalls. Similarly, business practices in the
commercial world led to the development of several security policy models such as the Clark-Wilson model and the
Chinese Wall model. These models in turn help the designers of security policies better understand and evaluate
the mechanisms and procedures needed to secure their sites.
The second goal is to emphasize that computer security and cryptography are different. Although cryptography is
an essential component of computer security, it is by no means the only component. Cryptography provides a
mechanism for performing specific functions, such as preventing unauthorized people from reading and altering
messages on a network. However, unless developers understand the context in which they are using cryptography,
and unless the assumptions underlying the protocol and the cryptographic mechanisms apply to the context, the
cryptography may not add to the security of the system. The canonical example is the use of cryptography to
secure communications between two low-security systems. If only trusted users can access the two systems,
cryptography protects messages in transit. But if untrusted users can access either system (through authorized
accounts or, more likely, by breaking in), the cryptography is not sufficient to protect the messages. The attackers
can read the messages at either endpoint.
The third goal is to demonstrate that computer security is not just a science but also an art. It is an art because no
system can be considered secure without an examination of how it is to be used. The definition of a "secure
computer" necessitates a statement of requirements and an expression of those requirements in the form of
authorized actions and authorized users. (A computer engaged in work at a university may be considered "secure"
for the purposes of the work done at the university. When moved to a military installation, that same system may
not provide sufficient control to be deemed "secure" for the purposes of the work done at that installation.) How
will people, as well as other computers, interact with the computer system? How clear and restrictive an interface
can a designer create without rendering the system unusable while trying to prevent unauthorized use or access to
the data or resources on the system?
Just as an artist paints his view of the world onto canvas, so does a designer of security features articulate his view
of the world of human/machine interaction in the security policy and mechanisms of the system. Two designers
may use entirely different designs to achieve the same creation, just as two artists may use different subjects to
achieve the same concept.
Computer security is also a science. Its theory is based on mathematical constructions, analyses, and proofs. Its
systems are built in accordance with the accepted practices of engineering. It uses inductive and deductive
reasoning to examine the security of systems from key axioms and to discover underlying principles. These
scientific principles can then be applied to untraditional situations and new theories, policies, and mechanisms.
Top
Philosophy
Key to understanding the problems that exist in computer security is a recognition that the problems are not new.
They are old problems, dating from the beginning of computer security (and, in fact, arising from parallel problems
in the noncomputer world). But the locus has changed as the field of computing has changed. Before the mid-
1980s, mainframe and mid-level computers dominated the market, and computer security problems and solutions
were phrased in terms of securing files or processes on a single system. With the rise of networking and the
Internet, the arena has changed. Workstations and servers, and the networking infrastructure that connects them,
now dominate the market. Computer security problems and solutions now focus on a networked environment.
However, if the workstations and servers, and the supporting network infrastructure, are viewed as a single
system, the models, theories, and problem statements developed for systems before the mid-1980s apply equally
well to current systems.
As an example, consider the issue of assurance. In the early period, assurance arose in several ways: formal
methods and proofs of correctness, validation of policy to requirements, and acquisition of data and programs from
trusted sources, to name a few. Those providing assurance analyzed a single system, the code on it, and the
sources (vendors and users) from which the code could be acquired to ensure that either the sources could be
trusted or the programs could be confined adequately to do minimal damage. In the later period, the same basic
principles and techniques apply, except that the scope of some has been greatly expanded (from a single system
and a small set of vendors to the world-wide Internet). The work on proof-carrying code, an exciting development
in which the proof that a downloadable program module satisfies a stated policy is incorporated into the program
itself,[2] is an example of this expansion. It extends the notion of a proof of consistency with a stated policy. It
advances the technology of the earlier period into the later period. But in order to understand it properly, one must
understand the ideas underlying the concept of proof-carrying code, and these ideas lie in the earlier period.
[2] Section 22.7.5.1 discusses proof-carrying code.
As another example, consider Saltzer and Schroeder's principles of secure design.[3] Enunciated in 1975, they
promote simplicity, confinement, and understanding. When security mechanisms grow too complex, attackers can
evade or bypass them. Many programmers and vendors are learning this when attackers break into their systems
and servers. The argument that the principles are old, and somehow outdated, rings hollow when the result of their
violation is a nonsecure system.
[3] Chapter 13 discusses these principles.
The work from the earlier period is sometimes cast in terms of systems that no longer exist and that differ in many
ways from modern systems. This does not vitiate the ideas and concepts, which also underlie the work done today.
Once these ideas and concepts are properly understood, applying them in a multiplicity of environments becomes
possible. Furthermore, the current mechanisms and technologies will become obsolete and of historical interest
themselves as new forms of computing arise, but the underlying principles will live on, to underlie the next
generation—indeed the next era—of computing.
The philosophy of this book is that certain key concepts underlie all of computer security, and that the study of all
parts of computer security enriches the understanding of all parts. Moreover, critical to an understanding of the
applications of security-related technologies and methodologies is an understanding of the theory underlying those
applications.
Advances in the theory of computer protection have illuminated the foundations of security systems. Issues of
abstract modeling, and modeling to meet specific environments, lead to systems designed to achieve a specific and
rewarding goal. Theorems about composability of policies[4] and the undecidability of the general security
question[5] have indicated the limits of what can be done. Much work and effort are continuing to extend the
borders of those limits.
[4] See Chapter 8, "Noninterference and Policy Composition."
[5] See Section 3.2, "Basic Results."
Application of these results has improved the quality of the security of the systems being protected. However, the
issue is how compatibly the assumptions of the model (and theory) conform to the environment to which the
theory is applied. Although our knowledge of how to apply these abstractions is continually increasing, we still have
difficulty correctly transposing the relevant information from a realistic setting to one in which analyses can then
proceed. Such abstraction often eliminates vital information. The omitted data may pertain to security in
nonobvious ways. Without this information, the analysis is flawed.
The practitioner needs to know both the theoretical and practical aspects of the art and science of computer
security. The theory demonstrates what is possible. The practical makes known what is feasible. The theoretician
needs to understand the constraints under which these theories are used, how their results are translated into
practical tools and methods, and how realistic are the assumptions underlying the theories. Computer Security: Art
and Science tries to meet these needs.
Unfortunately, no single work can cover all aspects of computer security, so this book focuses on those parts that
are, in the author's opinion, most fundamental and most pervasive. The mechanisms exemplify the applications of
these principles.
Top
Organization
The organization of this book reflects its philosophy. It begins with mathematical fundamentals and principles that
provide boundaries within which security can be modeled and analyzed effectively. The mathematics provides a
framework for expressing and analyzing the requirements of the security of a system. These policies constrain what
is allowed and what is not allowed. Mechanisms provide the ability to implement these policies. The degree to
which the mechanisms correctly implement the policies, and indeed the degree to which the policies themselves
meet the requirements of the organizations using the system, are questions of assurance. Exploiting failures in
policy, in implementation, and in assurance comes next, as well as mechanisms for providing information on the
attack. The book concludes with the applications of both theory and policy focused on realistic situations. This
natural progression emphasizes the development and application of the principles existent in computer security.
Part 1, "Introduction," describes what computer security is all about and explores the problems and challenges to
be faced. It sets the context for the remainder of the book.
Part 2, "Foundations," deals with basic questions such as how "security" can be clearly and functionally defined,
whether or not it is realistic, and whether or not it is decidable. If it is decidable, under what conditions is it
decidable, and if not, how must the definition be bounded in order to make it decidable?
Part 3, "Policy," probes the relationship between policy and security. The definition of "security" depends on policy.
In Part 3 we examine several types of policies, including the ever-present fundamental questions of trust, analysis
of policies, and the use of policies to constrain operations and transitions.
Part 4, "Implementation I: Cryptography," discusses cryptography and its role in security. It focuses on
applications and discusses issues such as key management and escrow, key distribution, and how cryptosystems
are used in networks. A quick study of authentication completes Part 4.
Part 5, "Implementation II: Systems," considers how to implement the requirements imposed by policies using
system-oriented techniques. Certain design principles are fundamental to effective security mechanisms. Policies
define who can act and how they can act, and so identity is a critical aspect of implementation. Mechanisms
implementing access control and flow control enforce various aspects of policies.
Part 6, "Assurance," presents methodologies and technologies for ascertaining how well a system, or a product,
meets its goals. After setting the background, to explain exactly what "assurance" is, the art of building systems to
meet varying levels of assurance is discussed. Formal verification methods play a role. Part 6 shows how the
progression of standards has enhanced our understanding of assurance techniques.
Part 7, "Special Topics," discusses some miscellaneous aspects of computer security. Malicious logic thwarts many
mechanisms. Despite our best efforts at high assurance, systems today are replete with vulnerabilities. Why? How
can a system be analyzed to detect vulnerabilities? What models might help us improve the state of the art? Given
these security holes, how can we detect attackers who exploit them? A discussion of auditing flows naturally into a
discussion of intrusion detection—a detection method for such attacks.
Part 8, "Practicum," presents examples of how to apply the principles discussed throughout the book. It begins with
networks and proceeds to systems, users, and programs. Each chapter states a desired policy and shows how to
translate that policy into a set of mechanisms and procedures that support the policy. Part 8 tries to demonstrate
that the material covered elsewhere can be, and should be, used in practice.
Each chapter in this book ends with a summary, descriptions of some research issues, and some suggestions for
further reading. The summary highlights the important ideas in the chapter. The research issues are current "hot
topics" or are topics that may prove to be fertile ground for advancing the state of the art and science of computer
security. Interested readers who wish to pursue the topics in any chapter in more depth can go to some of the
suggested readings. They expand on the material in the chapter or present other interesting avenues.
Top
Roadmap
This book is both a reference book and a textbook. Its audience is undergraduate and graduate students as well as
practitioners. This section offers some suggestions on approaching the book.
Dependencies
Chapter 1 is fundamental to the rest of the book and should be read first. After that, however, the reader need not
follow the chapters in order. Some of the dependencies among chapters are as follows.
Chapter 3 depends on Chapter 2 and requires a fair degree of mathematical maturity. Chapter 2, on the other
hand, does not. The material in Chapter 3 is for the most part not used elsewhere (although the existence of the
first section's key result, the undecidability theorem, is mentioned repeatedly). It can be safely skipped if the
interests of the reader lie elsewhere.
The chapters in Part 3 build on one another. The formalisms in Chapter 5 are called on in Chapters 19 and 20, but
nowhere else. Unless the reader intends to delve into the sections on theorem proving and formal mappings, the
formalisms may be skipped. The material in Chapter 8 requires a degree of mathematical maturity, and this
material is used sparingly elsewhere. Like Chapter 3, Chapter 8 can be skipped by the reader whose interests lie
elsewhere.
Chapters 9, 10, and 11 also build on one another in order. A reader who has encountered basic cryptography will
have an easier time with the material than one who has not, but the chapters do not demand the level of
mathematical experience that Chapters 3 and 8 require. Chapter 12 does not require material from Chapter 10 or
Chapter 11, but it does require material from Chapter 9.
Chapter 13 is required for all of Part 5. A reader who has studied operating systems at the undergraduate level will
have no trouble with Chapter 15. Chapter 14 uses the material in Chapter 11; Chapter 16 builds on material in
Chapters 5, 13, and 15; and Chapter 17 uses material in Chapters 4, 13, and 16.
Chapter 18 relies on information in Chapter 4. Chapter 19 builds on Chapters 5, 13, 15, and 18. Chapter 20
presents highly mathematical concepts and uses material from Chapters 18 and 19. Chapter 21 is based on
material in Chapters 5, 18, and 19; it does not require Chapter 20. For all of Part 5, a knowledge of software
engineering is very helpful.
Chapter 22 draws on ideas and information in Chapters 5, 6, 9, 13, 15, and 17 (and for Section 22.6, the reader
should read Section 3.1). Chapter 23 is self-contained, although it implicitly uses many ideas from assurance. It
also assumes a good working knowledge of compilers, operating systems, and in some cases networks. Many of
the flaws are drawn from versions of the UNIX operating system, or from Windows systems, and so a working
knowledge of either or both systems will make some of the material easier to understand. Chapter 24 uses
information from Chapter 4, and Chapter 25 uses material from Chapter 24.
The practicum chapters are self-contained and do not require any material beyond Chapter 1. However, they point
out relevant material in other sections that augments the information and (we hope) the reader's understanding of
that information.
Background
The material in this book is at the advanced undergraduate level. Throughout, we assume that the reader is
familiar with the basics of compilers and computer architecture (such as the use of the program stack) and
operating systems. The reader should also be comfortable with modular arithmetic (for the material on
cryptography). Some material, such as that on formal methods (Chapter 20) and the mathematical theory of
computer security (Chapter 3 and the formal presentation of policy models), requires considerable mathematical
maturity. Other specific recommended background is presented in the preceding section. Part 9, "End Matter,"
contains material that will be helpful to readers with backgrounds that lack some of the recommended material.
Examples are drawn from many systems. Many come from the UNIX operating system or variations of it (such as
Linux). Others come from the Windows family of systems. Familiarity with these systems will help the reader
understand many examples easily and quickly.
Undergraduate Level
An undergraduate class typically focuses on applications of theory and how students can use the material. The
specific arrangement and selection of material depends on the focus of the class, but all classes should cover some
basic material—notably that in Chapters 1, 9, and 13, as well as the notion of an access control matrix, which is
discussed in Sections 2.1 and 2.2.
Presentation of real problems and solutions often engages undergraduate students more effecively than
presentation of abstractions. The special topics and the practicum provide a wealth of practical problems and ways
to deal with them. This leads naturally to the deeper issues of policy, cryptography, noncryptographic mechanisms,
and assurance. The following are sections appropriate for nonmathematical undergraduate courses in these topics.
Policy: Sections 4.1 through 4.4 describe the notion of policy. The instructor should select one or two
examples from Sections 5.1, 5.2.1, 6.2, 6.4, 7.1.1, and 7.2, which describe several policy models
informally. Section 7.4 discusses role-based access control.
Cryptography: Key distribution is discussed in Sections 10.1 and 10.2, and a common form of public key
infrastructures (called PKIs) is discussed in Section 10.4.2. Section 11.1 points out common errors in using
cryptography. Section 11.3 shows how cryptography is used in networks, and the instructor should use one
of the protocols in Section 11.4 as an example. Chapter 12 offers a look at various forms of authentication,
including noncryptographic methods.
Noncryptographic mechanisms: Identity is the basis for many access control mechanisms. Sections 14.1
through 14.4 discuss identity on a system, and Section 14.6 discusses identity and anonymity on the Web.
Sections 15.1 and 15.2 explore two mechanisms for controlling access to files, and Section 15.4 discusses
the ring-based mechanism underlying the notion of multiple levels of privilege. If desired, the instructor
can cover sandboxes by using Sections 17.1 and 17.2, but because Section 17.2 uses material from
Sections 4.5 and 4.5.1, the instructor will need to go over those sections as well.
Assurance: Chapter 18 provides a basic introduction to the often overlooked topic of assurance.
Graduate Level
A typical introductory graduate class can focus more deeply on the subject than can an undergraduate class. Like
an undergraduate class, a graduate class should cover Chapters 1, 9, and 13. Also important are the undecidability
results in Sections 3.1 and 3.2, which require that Chapter 2 be covered. Beyond that, the instructor can choose
from a variety of topics and present them to whatever depth is appropriate. The following are sections suitable for
graduate study.
Policy models: Part 3 covers many common policy models both informally and formally. The formal
description is much easier to understand once the informal description is understood, so in all cases both
should be covered. The controversy in Section 5.4 is particularly illuminating to students who have not
considered the role of policy and the nature of a policy. Chapter 8 is a highly formal discussion of the
foundations of policy and is appropriate for students with experience in formal mathematics. Students
without such a background will find it quite difficult.
Cryptography: Part 4 focuses on the applications of cryptography, not on cryptography's mathematical
underpinnings.[6] It discusses areas of interest critical to the use of cryptography, such as key
management and some basic cryptographic protocols used in networking.
[6] The interested reader will find a number of books covering aspects of this subject [240, 588, 693,
700, 885, 894, 995].
Noncryptographic mechanisms: Issues of identity and certification are complex and generally poorly
understood. Section 14.5 covers these problems. Combining this with the discussion of identity on the Web
(Section 14.6) raises issues of trust and naming. Chapters 16 and 17 explore issues of information flow and
confining that flow.
Assurance: Traditionally, assurance is taught as formal methods, and Chapter 20 serves this purpose. In
practice, however, assurance is more often accomplished by using structured processes and techniques and
informal but rigorous arguments of justification, mappings, and analysis. Chapter 19 emphasizes these
topics. Chapter 21 discusses evaluation standards and relies heavily on the material in Chapters 18 and 19
and some of the ideas in Chapter 20.
Miscellaneous Topics: Section 22.6 presents a proof that the generic problem of determining if a generic
program is a computer virus is in fact undecidable. The theory of penetration studies in Section 23.2, and
the more formal approach in Section 23.5, illuminate the analysis of systems for vulnerabilities. If the
instructor chooses to cover intrusion detection (Chapter 25) in depth, it should be understood that this
discussion draws heavily on the material on auditing (Chapter 24).
Practicum: The practicum (Part 8) ties the material in the earlier part of the book to real-world examples
and emphasizes the applications of the theory and methodologies discussed earlier.
Practitioners
Practitioners in the field of computer security will find much to interest them. The table of contents and the index
will help them locate specific topics. A more general approach is to start with Chapter 1 and then proceed to Part 8,
the practicum. Each chapter has references to other sections of the text that explain the underpinnings of the
material. This will lead the reader to a deeper understanding of the reasons for the policies, settings,
configurations, and advice in the practicum. This approach also allows readers to focus on those topics that are of
most interest to them.
Top
Special Acknowledgment
Elisabeth Sullivan contributed the assurance part of this book. She wrote several drafts, all of which reflect her
extensive knowledge and experience in that aspect of computer security. I am particularly grateful to her for
contributing her real-world knowledge of how assurance is managed. Too often, books recount the mathematics of
assurance without recognizing that other aspects are equally important and more widely used. These other aspects
shine through in the assurance section, thanks to Liz. As if that were not enough, she made several suggestions
that improved the policy part of this book. I will always be grateful for her contribution, her humor, and especially
her friendship.
Top
Acknowledgments
Many people have contributed to this book. Peter Salus' suggestion first got me thinking about writing it, and Peter
put me in touch with Addison-Wesley. Midway through the writing, Blaine Burnham reviewed the completed
portions and the proposed topics, and suggested that they be reorganized in several ways. The current
organization of the book grew from his suggestions. Marvin Schaefer reviewed parts of the book with a keen eye,
made suggestions that improved many parts, and encouraged me at the end. I thank these three for their
contributions.
Many others contributed to this book in various ways. Special thanks to Jim Alves-Foss, Bill Arbaugh, Andrew
Arcilla, Rebecca Bace, Belinda Bashore, Terry Brugger, Michael Clifford, Crispin Cowan, Dimitri DeFigueiredo,
Jeremy Frank, Robert Fourney, Ron Gove, Jesper Johansson, Calvin Ko, Karl Levitt, Gary McGraw, Alexander Meau,
Nasir Memon, Mark Morrissey, Stephen Northcutt, Holly Pang, Sung Park, Ashwini Raina, Brennen Reynolds,
Christoph Schuba, Jonathan Shapiro, Clay Shields, Tom Walcott, Dan Watson, and Chris Wee, and to everyone in
my computer security classes, who (knowingly or unknowingly) helped me develop and test this material.
The Addison-Wesley folks, Kathleen Billus, Susannah Buzard, Bernie Gaffney, Amy Fleischer, Helen Goldstein, Tom
Stone, Asdis Thorsteinsson, and most especially my editor, Peter Gordon, were incredibly patient and helpful,
despite fears that this book would never materialize. The fact that it did so is in great measure attributable to their
hard work and encouragement. I also thank the production people at Argosy, especially Beatriz Valdés and Craig
Kirkpatrick, for their wonderful work.
Dorothy Denning, my advisor in graduate school, guided me through the maze of computer security when I was
just beginning. Peter Denning, Barry Leiner, Karl Levitt, Peter Neumann, Marvin Schaefer, Larry Snyder, and
several others influenced my approach to the subject. I hope this work reflects in some small way what they gave
to me and passes a modicum of it along to my readers.
I also thank my parents, Leonard Bishop and Linda Allen. My father, a writer, gave me some useful tips on writing,
which I tried to follow. My mother, a literary agent, helped me understand the process of getting the book
published, and supported me throughout.
Finally, I would like to thank my family for their support throughout the writing. Sometimes they wondered if I
would ever finish. My wife Holly and our children Steven, David, and Caroline were very patient and understanding
and made sure I had time to work on the book. Our oldest daughter Heidi and her husband Mike also provided
much love and encouragement and the most wonderful distraction: our grandson—Skyler. To all, my love and
gratitude.
Top
Part 1: Introduction
Writers say "To write a good book, first tell them what you are going to tell them, then tell them,
then tell them what you told them." This is the "what we're going to tell you" part. Chapter 1, "An
Overview of Computer Security," presents the underpinnings of computer security and an overview
of the important issues to place them in context. It begins with a discussion of what computer
security is and how threats are connected to security services. The combination of desired services
makes up a policy, and mechanisms enforce the policy. All rely on underlying assumptions, and the
systems built on top of these assumptions lead to issues of assurance. Finally, the operational and
human factors affect the mechanisms used as well as the policy.
Top
Chapter 1. An Overview of Computer Security
ANTONIO: Whereof what's past is prologue, what to come
In yours and my discharge.
—The Tempest, II, i, 257–258.
This chapter presents the basic concepts of computer security. The remainder of the book will elaborate on these
concepts in order to reveal the logic underlying the principles of these concepts.
We begin with basic security-related services that protect against threats to the security of the system. The next
section discusses security policies that identify the threats and define the requirements for ensuring a secure
system. Security mechanisms detect and prevent attacks and recover from those that succeed. Analyzing the
security of a system requires an understanding of the mechanisms that enforce the security policy. It also requires
a knowledge of the related assumptions and trust, which lead to the threats and the degree to which they may be
realized. Such knowledge allows one to design better mechanisms and policies to neutralize these threats. This
process leads to risk analysis. Human beings are the weakest link in the security mechanisms of any system.
Therefore, policies and procedures must take people into account. This chapter discusses each of these topics.
Top
1.1 The Basic Components
Computer security rests on confidentiality, integrity, and availability. The interpretations of these three aspects
vary, as do the contexts in which they arise. The interpretation of an aspect in a given environment is dictated by
the needs of the individuals, customs, and laws of the particular organization.
1.1.1 Confidentiality
Confidentiality is the concealment of information or resources. The need for keeping information secret arises from
the use of computers in sensitive fields such as government and industry. For example, military and civilian
institutions in the government often restrict access to information to those who need that information. The first
formal work in computer security was motivated by the military's attempt to implement controls to enforce a "need
to know" principle. This principle also applies to industrial firms, which keep their proprietary designs secure lest
their competitors try to steal the designs. As a further example, all types of institutions keep personnel records
secret.
Access control mechanisms support confidentiality. One access control mechanism for preserving confidentiality is
cryptography, which scrambles data to make it incomprehensible. A cryptographic key controls access to the
unscrambled data, but then the cryptographic key itself becomes another datum to be protected.
EXAMPLE: Enciphering an income tax return will prevent anyone from reading it. If the owner needs to
see the return, it must be deciphered. Only the possessor of the cryptographic key can enter it into a
deciphering program. However, if someone else can read the key when it is entered into the program,
the confidentiality of the tax return has been compromised.
Other system-dependent mechanisms can prevent processes from illicitly accessing information. Unlike enciphered
data, however, data protected only by these controls can be read when the controls fail or are bypassed. Then their
advantage is offset by a corresponding disadvantage. They can protect the secrecy of data more completely than
cryptography, but if they fail or are evaded, the data becomes visible.
Confidentiality also applies to the existence of data, which is sometimes more revealing than the data itself. The
precise number of people who distrust a politician may be less important than knowing that such a poll was taken
by the politician's staff. How a particular government agency harassed citizens in its country may be less important
than knowing that such harassment occurred. Access control mechanisms sometimes conceal the mere existence of
data, lest the existence itself reveal information that should be protected.
Resource hiding is another important aspect of confidentiality. Sites often wish to conceal their configuration as
well as what systems they are using; organizations may not wish others to know about specific equipment
(because it could be used without authorization or in inappropriate ways), and a company renting time from a
service provider may not want others to know what resources it is using. Access control mechanisms provide these
capabilities as well.
All the mechanisms that enforce confidentiality require supporting services from the system. The assumption is
that the security services can rely on the kernel, and other agents, to supply correct data. Thus, assumptions and
trust underlie confidentiality mechanisms.
1.1.2 Integrity
Integrity refers to the trustworthiness of data or resources, and it is usually phrased in terms of preventing
improper or unauthorized change. Integrity includes data integrity (the content of the information) and origin
integrity (the source of the data, often called authentication). The source of the information may bear on its
accuracy and credibility and on the trust that people place in the information. This dichotomy illustrates the
principle that the aspect of integrity known as credibility is central to the proper functioning of a system. We will
return to this issue when discussing malicious logic.
EXAMPLE: A newspaper may print information obtained from a leak at the White House but attribute it
to the wrong source. The information is printed as received (preserving data integrity), but its source
is incorrect (corrupting origin integrity).
Integrity mechanisms fall into two classes: prevention mechanisms and detection mechanisms.
Prevention mechanisms seek to maintain the integrity of the data by blocking any unauthorized attempts to change
the data or any attempts to change the data in unauthorized ways. The distinction between these two types of
attempts is important. The former occurs when a user tries to change data which she has no authority to change.
The latter occurs when a user authorized to make certain changes in the data tries to change the data in other
ways. For example, suppose an accounting system is on a computer. Someone breaks into the system and tries to
modify the accounting data. Then an unauthorized user has tried to violate the integrity of the accounting
database. But if an accountant hired by the firm to maintain its books tries to embezzle money by sending it
overseas and hiding the transactions, a user (the accountant) has tried to change data (the accounting data) in
unauthorized ways (by moving it to a Swiss bank account). Adequate authentication and access controls will
generally stop the break-in from the outside, but preventing the second type of attempt requires very different
controls.
Detection mechanisms do not try to prevent violations of integrity; they simply report that the data's integrity is no
longer trustworthy. Detection mechanisms may analyze system events (user or system actions) to detect problems
or (more commonly) may analyze the data itself to see if required or expected constraints still hold. The
mechanisms may report the actual cause of the integrity violation (a specific part of a file was altered), or they
may simply report that the file is now corrupt.
Working with integrity is very different from working with confidentiality. With confidentiality, the data is either
compromised or it is not, but integrity includes both the correctness and the trustworthiness of the data. The origin
of the data (how and from whom it was obtained), how well the data was protected before it arrived at the current
machine, and how well the data is protected on the current machine all affect the integrity of the data. Thus,
evaluating integrity is often very difficult, because it relies on assumptions about the source of the data and about
trust in that source—two underpinnings of security that are often overlooked.
1.1.3 Availability
Availability refers to the ability to use the information or resource desired. Availability is an important aspect of
reliability as well as of system design because an unavailable system is at least as bad as no system at all. The
aspect of availability that is relevant to security is that someone may deliberately arrange to deny access to data or
to a service by making it unavailable. System designs usually assume a statistical model to analyze expected
patterns of use, and mechanisms ensure availability when that statistical model holds. Someone may be able to
manipulate use (or parameters that control use, such as network traffic) so that the assumptions of the statistical
model are no longer valid. This means that the mechanisms for keeping the resource or data available are working
in an environment for which they were not designed. As a result, they will often fail.
EXAMPLE: Suppose Anne has compromised a bank's secondary system server, which supplies bank
account balances. When anyone else asks that server for information, Anne can supply any
information she desires. Merchants validate checks by contacting the bank's primary balance server. If
a merchant gets no response, the secondary server will be asked to supply the data. Anne's colleague
prevents merchants from contacting the primary balance server, so all merchant queries go to the
secondary server. Anne will never have a check turned down, regardless of her actual account
balance. Notice that if the bank had only one server (the primary one), this scheme would not work.
The merchant would be unable to validate the check.
Attempts to block availability, called denial of service attacks, can be the most difficult to detect, because the
analyst must determine if the unusual access patterns are attributable to deliberate manipulation of resources or of
environment. Complicating this determination is the nature of statistical models. Even if the model accurately
describes the environment, atypical events simply contribute to the nature of the statistics. A deliberate attempt to
make a resource unavailable may simply look like, or be, an atypical event. In some environments, it may not even
appear atypical.
Top
1.2 Threats
A threat is a potential violation of security. The violation need not actually occur for there to be a threat. The fact
that the violation might occur means that those actions that could cause it to occur must be guarded against (or
prepared for). Those actions are called attacks. Those who execute such actions, or cause them to be executed, are
called attackers.
The three security services—confidentiality, integrity, and availability—counter threats to the security of a system.
Shirey [916] divides threats into four broad classes: disclosure, or unauthorized access to information; deception,
or acceptance of false data; disruption, or interruption or prevention of correct operation; and usurpation, or
unauthorized control of some part of a system. These four broad classes encompass many common threats.
Because the threats are ubiquitous, an introductory discussion of each one will present issues that recur
throughout the study of computer security.
Snooping, the unauthorized interception of information, is a form of disclosure. It is passive, suggesting simply that
some entity is listening to (or reading) communications or browsing through files or system information.
Wiretapping, or passive wiretapping, is a form of snooping in which a network is monitored. (It is called
"wiretapping" because of the "wires" that compose the network, although the term is used even if no physical
wiring is involved.) Confidentiality services counter this threat.
Modification or alteration, an unauthorized change of information, covers three classes of threats. The goal may be
deception, in which some entity relies on the modified data to determine which action to take, or in which incorrect
information is accepted as correct and is released. If the modified data controls the operation of the system, the
threats of disruption and usurpation arise. Unlike snooping, modification is active; it results from an entity
changing information. Active wiretapping is a form of modification in which data moving across a network is
altered; the term "active" distinguishes it from snooping ("passive" wiretapping). An example is the man-in-the-
middle attack, in which an intruder reads messages from the sender and sends (possibly modified) versions to the
recipient, in hopes that the recipient and sender will not realize the presence of the intermediary. Integrity services
counter this threat.
Masquerading or spoofing, an impersonation of one entity by another, is a form of both deception and usurpation.
It lures a victim into believing that the entity with which it is communicating is a different entity. For example, if a
user tries to log into a computer across the Internet but instead reaches another computer that claims to be the
desired one, the user has been spoofed. Similarly, if a user tries to read a file, but an attacker has arranged for the
user to be given a different file, another spoof has taken place. This may be a passive attack (in which the user
does not attempt to authenticate the recipient, but merely accesses it), but it is usually an active attack (in which
the masquerader issues responses to mislead the user about its identity). Although primarily deception, it is often
used to usurp control of a system by an attacker impersonating an authorized manager or controller. Integrity
services (called "authentication services" in this context) counter this threat.
Some forms of masquerading may be allowed. Delegation occurs when one entity authorizes a second entity to
perform functions on its behalf. The distinctions between delegation and masquerading are important. If Susan
delegates to Thomas the authority to act on her behalf, she is giving permission for him to perform specific actions
as though she were performing them herself. All parties are aware of the delegation. Thomas will not pretend to be
Susan; rather, he will say, "I am Thomas and I have authority to do this on Susan's behalf." If asked, Susan will
verify this. On the other hand, in a masquerade, Thomas will pretend to be Susan. No other parties (including
Susan) will be aware of the masquerade, and Thomas will say, "I am Susan." Should anyone discover that he or
she is dealing with Thomas and ask Susan about it, she will deny that she authorized Thomas to act on her behalf.
In terms of security, masquerading is a violation of security, whereas delegation is not.
Repudiation of origin, a false denial that an entity sent (or created) something, is a form of deception. For
example, suppose a customer sends a letter to a vendor agreeing to pay a large amount of money for a product.
The vendor ships the product and then demands payment. The customer denies having ordered the product and by
law is therefore entitled to keep the unsolicited shipment without payment. The customer has repudiated the origin
of the letter. If the vendor cannot prove that the letter came from the customer, the attack succeeds. A variant of
this is denial by a user that he created specific information or entities such as files. Integrity mechanisms cope with
this threat.
Denial of receipt, a false denial that an entity received some information or message, is a form of deception.
Suppose a customer orders an expensive product, but the vendor demands payment before shipment. The
customer pays, and the vendor ships the product. The customer then asks the vendor when he will receive the
product. If the customer has already received the product, the question constitutes a denial of receipt attack. The
vendor can defend against this attack only by proving that the customer did, despite his denials, receive the
product. Integrity and availability mechanisms guard against these attacks.
Delay, a temporary inhibition of a service, is a form of usurpation, although it can play a supporting role in
deception. Typically, delivery of a message or service requires some time t; if an attacker can force the delivery to
take more than time t, the attacker has successfully delayed delivery. This requires manipulation of system control
structures, such as network components or server components, and hence is a form of usurpation. If an entity is
waiting for an authorization message that is delayed, it may query a secondary server for the authorization. Even
though the attacker may be unable to masquerade as the primary server, she might be able to masquerade as that
secondary server and supply incorrect information. Availability mechanisms can thwart this threat.
Denial of service, a long-term inhibition of service, is a form of usurpation, although it is often used with other
mechanisms to deceive. The attacker prevents a server from providing a service. The denial may occur at the
source (by preventing the server from obtaining the resources needed to perform its function), at the destination
(by blocking the communications from the server), or along the intermediate path (by discarding messages from
either the client or the server, or both). Denial of service poses the same threat as an infinite delay. Availability
mechanisms counter this threat.
Denial of service or delay may result from direct attacks or from nonsecurity-related problems. From our point of
view, the cause and result are important; the intention underlying them is not. If delay or denial of service
compromises system security, or is part of a sequence of events leading to the compromise of a system, then we
view it as an attempt to breach system security. But the attempt may not be deliberate; indeed, it may be the
product of environmental characteristics rather than specific actions of an attacker.
Top
1.3 Policy and Mechanism
Critical to our study of security is the distinction between policy and mechanism.
Definition 1–1. A security policy is a statement of what is, and what is not, allowed.
Definition 1–2. A security mechanism is a method, tool, or procedure for enforcing a security
policy.
Mechanisms can be nontechnical, such as requiring proof of identity before changing a password; in fact, policies
often require some procedural mechanisms that technology cannot enforce.
As an example, suppose a university's computer science laboratory has a policy that prohibits any student from
copying another student's homework files. The computer system provides mechanisms for preventing others from
reading a user's files. Anna fails to use these mechanisms to protect her homework files, and Bill copies them. A
breach of security has occurred, because Bill has violated the security policy. Anna's failure to protect her files does
not authorize Bill to copy them.
In this example, Anna could easily have protected her files. In other environments, such protection may not be
easy. For example, the Internet provides only the most rudimentary security mechanisms, which are not adequate
to protect information sent over that network. Nevertheless, acts such as the recording of passwords and other
sensitive information violate an implicit security policy of most sites (specifically, that passwords are a user's
confidential property and cannot be recorded by anyone).
Policies may be presented mathematically, as a list of allowed (secure) and disallowed (nonsecure) states. For our
purposes, we will assume that any given policy provides an axiomatic description of secure states and nonsecure
states. In practice, policies are rarely so precise; they normally describe in English what users and staff are allowed
to do. The ambiguity inherent in such a description leads to states that are not classified as "allowed" or
"disallowed." For example, consider the homework policy discussed above. If someone looks through another
user's directory without copying homework files, is that a violation of security? The answer depends on site custom,
rules, regulations, and laws, all of which are outside our focus and may change over time.
When two different sites communicate or cooperate, the entity they compose has a security policy based on the
security policies of the two entities. If those policies are inconsistent, either or both sites must decide what the
security policy for the combined site should be. The inconsistency often manifests itself as a security breach. For
example, if proprietary documents were given to a university, the policy of confidentiality in the corporation would
conflict with the more open policies of most universities. The university and the company must develop a mutual
security policy that meets both their needs in order to produce a consistent policy. When the two sites
communicate through an independent third party, such as an Internet Service Provider, the complexity of the
situation grows rapidly.
1.3.1 Goals of Security
Given a security policy's specification of "secure" and "nonsecure" actions, these security mechanisms can prevent
the attack, detect the attack, or recover from the attack. The strategies may be used together or separately.
Prevention means that an attack will fail. For example, if one attempts to break into a host over the Internet and
that host is not connected to the Internet, the attack has been prevented. Typically, prevention involves
implementation of mechanisms that users cannot override and that are trusted to be implemented in a correct,
unalterable way, so that the attacker cannot defeat the mechanism by changing it. Preventative mechanisms often
are very cumbersome and interfere with system use to the point that they hinder normal use of the system. But
some simple preventative mechanisms, such as passwords (which aim to prevent unauthorized users from
accessing the system), have become widely accepted. Prevention mechanisms can prevent compromise of parts of
the system; once in place, the resource protected by the mechanism need not be monitored for security problems,
at least in theory.
Detection is most useful when an attack cannot be prevented, but it can also indicate the effectiveness of
preventative measures. Detection mechanisms accept that an attack will occur; the goal is to determine that an
attack is underway, or has occurred, and report it. The attack may be monitored, however, to provide data about
its nature, severity, and results. Typical detection mechanisms monitor various aspects of the system, looking for
actions or information indicating an attack. A good example of such a mechanism is one that gives a warning when
a user enters an incorrect password three times. The login may continue, but an error message in a system log
reports the unusually high number of mistyped passwords. Detection mechanisms do not prevent compromise of
parts of the system, which is a serious drawback. The resource protected by the detection mechanism is
continuously or periodically monitored for security problems.
Recovery has two forms. The first is to stop an attack and to assess and repair any damage caused by that attack.
As an example, if the attacker deletes a file, one recovery mechanism would be to restore the file from backup
tapes. In practice, recovery is far more complex, because the nature of each attack is unique. Thus, the type and
extent of any damage can be difficult to characterize completely. Moreover, the attacker may return, so recovery
involves identification and fixing of the vulnerabilities used by the attacker to enter the system. In some cases,
retaliation (by attacking the attacker's system or taking legal steps to hold the attacker accountable) is part of
recovery. In all these cases, the system's functioning is inhibited by the attack. By definition, recovery requires
resumption of correct operation.
In a second form of recovery, the system continues to function correctly while an attack is underway. This type of
recovery is quite difficult to implement because of the complexity of computer systems. It draws on techniques of
fault tolerance as well as techniques of security and is typically used in safety-critical systems. It differs from the
first form of recovery, because at no point does the system function incorrectly. However, the system may disable
nonessential functionality. Of course, this type of recovery is often implemented in a weaker form whereby the
system detects incorrect functioning automatically and then corrects (or attempts to correct) the error.
Top
1.4 Assumptions and Trust
How do we determine if the policy correctly describes the required level and type of security for the site? This
question lies at the heart of all security, computer and otherwise. Security rests on assumptions specific to the type
of security required and the environment in which it is to be employed.
EXAMPLE: Opening a door lock requires a key. The assumption is that the lock is secure against lock
picking. This assumption is treated as an axiom and is made because most people would require a key
to open a door lock. A good lock picker, however, can open a lock without a key. Hence, in an
environment with a skilled, untrustworthy lock picker, the assumption is wrong and the consequence
invalid.
If the lock picker is trustworthy, the assumption is valid. The term "trustworthy" implies that the lock picker will not
pick a lock unless the owner of the lock authorizes the lock picking. This is another example of the role of trust. A
well-defined exception to the rules provides a "back door" through which the security mechanism (the locks) can
be bypassed. The trust resides in the belief that this back door will not be used except as specified by the policy. If
it is used, the trust has been misplaced and the security mechanism (the lock) provides no security.
Like the lock example, a policy consists of a set of axioms that the policy makers believe can be enforced.
Designers of policies always make two assumptions. First, the policy correctly and unambiguously partitions the set
of system states into "secure" and "nonsecure" states. Second, the security mechanisms prevent the system from
entering a "nonsecure" state. If either assumption is erroneous, the system will be nonsecure.
These two assumptions are fundamentally different. The first assumption asserts that the policy is a correct
description of what constitutes a "secure" system. For example, a bank's policy may state that officers of the bank
are authorized to shift money among accounts. If a bank officer puts $100,000 in his account, has the bank's
security been violated? Given the aforementioned policy statement, no, because the officer was authorized to move
the money. In the "real world," that action would constitute embezzlement, something any bank would consider a
security violation.
The second assumption says that the security policy can be enforced by security mechanisms. These mechanisms
are either secure, precise, or broad. Let P be the set of all possible states. Let Q be the set of secure states (as
specified by the security policy). Let the security mechanisms restrict the system to some set of states R (thus, R
P). Then we have the following definition.
Definition 1–3. A security mechanism is secure if R Q; it is precise if R = Q; and it is broad if
there are states r such that r R and r Q.
Ideally, the union of all security mechanisms active on a system would produce a single precise mechanism (that
is, R = A). In practice, security mechanisms are broad; they allow the system to enter nonsecure states. We will
revisit this topic when we explore policy formulation in more detail.
Trusting that mechanisms work requires several assumptions.
1. Each mechanism is designed to implement one or more parts of the security policy.
2. The union of the mechanisms implements all aspects of the security policy.
3. The mechanisms are implemented correctly.
4. The mechanisms are installed and administered correctly.
Because of the importance and complexity of trust and of assumptions, we will revisit this topic repeatedly and in
various guises throughout this book.
Top
1.5 Assurance
Trust cannot be quantified precisely. System specification, design, and implementation can provide a basis for
determining "how much" to trust a system. This aspect of trust is called assurance. It is an attempt to provide a
basis for bolstering (or substantiating or specifying) how much one can trust a system.
EXAMPLE: In the United States, aspirin from a nationally known and reputable manufacturer, delivered
to the drugstore in a safety-sealed container, and sold with the seal still in place, is considered
trustworthy by most people. The bases for that trust are as follows.
The testing and certification of the drug (aspirin) by the Food and Drug Administration. The
FDA has jurisdiction over many types of medicines and allows medicines to be marketed only
if they meet certain clinical standards of usefulness.
The manufacturing standards of the company and the precautions it takes to ensure that the
drug is not contaminated. National and state regulatory commissions and groups ensure that
the manufacture of the drug meets specific acceptable standards.
The safety seal on the bottle. To insert dangerous chemicals into a safety-sealed bottle
without damaging the seal is very difficult.
The three technologies (certification, manufacturing standards, and preventative sealing) provide
some degree of assurance that the aspirin is not contaminated. The degree of trust the purchaser has
in the purity of the aspirin is a result of these three processes.
In the 1980s, drug manufacturers met two of the criteria above, but none used safety seals.[1] A
series of "drug scares" arose when a well-known manufacturer's medicines were contaminated after
manufacture but before purchase. The manufacturer promptly introduced safety seals to assure its
customers that the medicine in the container was the same as when it was shipped from the
manufacturing plants.
[1] Many used childproof caps, but they prevented only young children (and some adults) from opening the
bottles. They were not designed to protect the medicine from malicious adults.
Assurance in the computer world is similar. It requires specific steps to ensure that the computer will function
properly. The sequence of steps includes detailed specifications of the desired (or undesirable) behavior; an
analysis of the design of the hardware, software, and other components to show that the system will not violate
the specifications; and arguments or proofs that the implementation, operating procedures, and maintenance
procedures will produce the desired behavior.
Definition 1–4. A system is said to satisfy a specification if the specification correctly states how
the system will function.
This definition also applies to design and implementation satisfying a specification.
1.5.1 Specification
A specification is a (formal or informal) statement of the desired functioning of the system. It can be highly
mathematical, using any of several languages defined for that purpose. It can also be informal, using, for example,
English to describe what the system should do under certain conditions. The specification can be low-level,
combining program code with logical and temporal relationships to specify ordering of events. The defining quality
is a statement of what the system is allowed to do or what it is not allowed to do.
EXAMPLE: A company is purchasing a new computer for internal use. They need to trust the system to
be invulnerable to attack over the Internet. One of their (English) specifications would read "The
system cannot be attacked over the Internet."
Specifications are used not merely in security but also in systems designed for safety, such as medical technology.
They constrain such systems from performing acts that could cause harm. A system that regulates traffic lights
must ensure that pairs of lights facing the same way turn red, green, and yellow at the same time and that at most
one set of lights facing cross streets at an intersection is green.
A major part of the derivation of specifications is determination of the set of requirements relevant to the system's
planned use. Section 1.6 discusses the relationship of requirements to security.
1.5.2 Design
The design of a system translates the specifications into components that will implement them. The design is said
to satisfy the specifications if, under all relevant circumstances, the design will not permit the system to violate
those specifications.
EXAMPLE: A design of the computer system for the company mentioned above had no network
interface cards, no modem cards, and no network drivers in the kernel. This design satisfied the
specification because the system would not connect to the Internet. Hence it could not be attacked
over the Internet.
An analyst can determine whether a design satisfies a set of specifications in several ways. If the specifications and
designs are expressed in terms of mathematics, the analyst must show that the design formulations are consistent
with the specifications. Although much of the work can be done mechanically, a human must still perform some
analyses and modify components of the design that violate specifications (or, in some cases, components that
cannot be shown to satisfy the specifications). If the specifications and design do not use mathematics, then a
convincing and compelling argument should be made. Most often, the specifications are nebulous and the
arguments are half-hearted and unconvincing or provide only partial coverage. The design depends on assumptions
about what the specifications mean. This leads to vulnerabilities, as we will see.
1.5.3 Implementation
Given a design, the implementation creates a system that satisfies that design. If the design also satisfies the
specifications, then by transitivity the implementation will also satisfy the specifications.
The difficulty at this step is the complexity of proving that a program correctly implements the design and, in turn,
the specifications.
Definition 1–5. A program is correct if its implementation performs as specified.
Proofs of correctness require each line of source code to be checked for mathematical correctness. Each line is seen
as a function, transforming the input (constrained by preconditions) into some output (constrained by
postconditions derived from the function and the preconditions). Each routine is represented by the composition of
the functions derived from the lines of code making up the routine. Like those functions, the function corresponding
to the routine has inputs and outputs, constrained by preconditions and postconditions, respectively. From the
combination of routines, programs can be built and formally verified. One can apply the same techniques to sets of
programs and thus verify the correctness of a system.
There are three difficulties in this process. First, the complexity of programs makes their mathematical verification
difficult. Aside from the intrinsic difficulties, the program itself has preconditions derived from the environment of
the system. These preconditions are often subtle and difficult to specify, but unless the mathematical formalism
captures them, the program verification may not be valid because critical assumptions may be wrong. Second,
program verification assumes that the programs are compiled correctly, linked and loaded correctly, and executed
correctly. Hardware failure, buggy code, and failures in other tools may invalidate the preconditions. A compiler
that incorrectly compiles
x := x + 1
to
move x to regA
subtract 1 from contents of regA
move contents of regA to x
would invalidate the proof statement that the value of x after the line of code is 1 more than the value of x before
the line of code. This would invalidate the proof of correctness. Third, if the verification relies on conditions on the
input, the program must reject any inputs that do not meet those conditions. Otherwise, the program is only
partially verified.
Because formal proofs of correctness are so time-consuming, a posteriori verification techniques known as testing
have become widespread. During testing, the tester executes the program (or portions of it) on data to determine
if the output is what it should be and to understand how likely the program is to contain an error. Testing
techniques range from supplying input to ensure that all execution paths are exercised to introducing errors into
the program and determining how they affect the output to stating specifications and testing the program to see if
it satisfies the specifications. Although these techniques are considerably simpler than the more formal methods,
they do not provide the same degree of assurance that formal methods do. Furthermore, testing relies on test
procedures and documentation, errors in either of which could invalidate the testing results.
Although assurance techniques do not guarantee correctness or security, they provide a firm basis for assessing
what one must trust in order to believe that a system is secure. Their value is in eliminating possible, and common,
sources of error and forcing designers to define precisely what the system is to do.
Top
1.6 Operational Issues
Any useful policy and mechanism must balance the benefits of the protection against the cost of designing,
implementing, and using the mechanism. This balance can be determined by analyzing the risks of a security
breach and the likelihood of it occurring. Such an analysis is, to a degree, subjective, because in very few
situations can risks be rigorously quantified. Complicating the analysis are the constraints that laws, customs, and
society in general place on the acceptability of security procedures and mechanisms; indeed, as these factors
change, so do security mechanisms and, possibly, security policies.
1.6.1 Cost-Benefit Analysis
Like any factor in a complex system, the benefits of computer security are weighed against their total cost
(including the additional costs incurred if the system is compromised). If the data or resources cost less, or are of
less value, than their protection, adding security mechanisms and procedures is not cost-effective because the data
or resources can be reconstructed more cheaply than the protections themselves. Unfortunately, this is rarely the
case.
EXAMPLE: A database provides salary information to a second system that prints checks. If the data in
the database is altered, the company could suffer grievous financial loss; hence, even a cursory cost-
benefit analysis would show that the strongest possible integrity mechanisms should protect the data
in the database.
Now suppose the company has several branch offices, and every day the database downloads a copy
of the data to each branch office. The branch offices use the data to recommend salaries for new
employees. However, the main office makes the final decision using the original database (not one of
the copies). In this case, guarding the integrity of the copies is not particularly important, because
branch offices cannot make any financial decisions based on the data in their copies. Hence, the
company cannot suffer any financial loss.
Both of these situations are extreme situations in which the analysis is clear-cut. As an example of a situation in
which the analysis is less clear, consider the need for confidentiality of the salaries in the database. The officers of
the company must decide the financial cost to the company should the salaries be disclosed, including potential
loss from lawsuits (if any); changes in policies, procedures, and personnel; and the effect on future business.
These are all business-related judgments, and determining their value is part of what company officers are paid to
do.
Overlapping benefits are also a consideration. Suppose the integrity protection mechanism can be augmented very
quickly and cheaply to provide confidentiality. Then the cost of providing confidentiality is much lower. This shows
that evaluating the cost of a particular security service depends on the mechanism chosen to implement it and on
the mechanisms chosen to implement other security services. The cost-benefit analysis should take into account as
many mechanisms as possible. Adding security mechanisms to an existing system is often more expensive (and,
incidentally, less effective) than designing them into the system in the first place.
1.6.2 Risk Analysis
To determine whether an asset should be protected, and to what level, requires analysis of the potential threats
against that asset and the likelihood that they will materialize. The level of protection is a function of the
probability of an attack occurring and the effects of the attack should it succeed. If an attack is unlikely, protecting
against it has a lower priority than protecting against a likely one. If the unlikely attack would cause long delays in
the company's production of widgets but the likely attack would be only a nuisance, then more effort should be put
into preventing the unlikely attack. The situations between these extreme cases are far more subjective.
Let's revisit our company with the salary database that transmits salary information over a network to a second
computer that prints employees' checks. The data is stored on the database system and then moved over the
network to the second system. Hence, the risk of unauthorized changes in the data occurs in three places: on the
database system, on the network, and on the printing system. If the network is a local (company-wide) one and no
wide area networks are accessible, the threat of attackers entering the systems is confined to untrustworthy
internal personnel. If, however, the network is connected to the Internet, the risk of geographically distant
attackers attempting to intrude is substantial enough to warrant consideration.
This example illustrates some finer points of risk analysis. First, risk is a function of environment. Attackers from a
foreign country are not a threat to the company when the computer is not connected to the Internet. If foreign
attackers wanted to break into the system, they would need physically to enter the company (and would cease to
be "foreign" because they would then be "local"). But if the computer is connected to the Internet, foreign
attackers become a threat because they can attack over the Internet. An additional, less tangible issue is the faith
in the company. If the company is not able to meet its payroll because it does not know whom it is to pay, the
company will lose the faith of its employees. It may be unable to hire anyone, because the people hired would not
be sure they would get paid. Investors would not fund the company because of the likelihood of lawsuits by unpaid
employees. The risk arises from the environments in which the company functions.
Second, the risks change with time. If a company's network is not connected to the Internet, there seems to be no
risk of attacks from other hosts on the Internet. However, despite any policies to the contrary, someone could
connect a modem to one of the company computers and connect to the Internet through the modem. Should this
happen, any risk analysis predicated on isolation from the Internet would no longer be accurate. Although policies
can forbid the connection of such a modem and procedures can be put in place to make such connection difficult,
unless the responsible parties can guarantee that no such modem will ever be installed, the risks can change.
Third, many risks are quite remote but still exist. In the modem example, the company has sought to minimize the
risk of an Internet connection. Hence, this risk is "acceptable" but not nonexistent. As a practical matter, one does
not worry about acceptable risks; instead, one worries that the risk will become unacceptable.
Finally, the problem of "analysis paralysis" refers to making risk analyses with no effort to act on those analyses.
To change the example slightly, suppose the company performs a risk analysis. The executives decide that they are
not sure if all risks have been found, so they order a second study to verify the first. They reconcile the studies
then wait for some time to act on these analyses. At that point, the security officers raise the objection that the
conditions in the workplace are no longer those that held when the original risk analyses were done. The analysis is
repeated. But the company cannot decide how to ameliorate the risks, so it waits until a plan of action can be
developed, and the process continues. The point is that the company is paralyzed and cannot act on the risks it
faces.
1.6.3 Laws and Customs
Laws restrict the availability and use of technology and affect procedural controls. Hence, any policy and any
selection of mechanisms must take into account legal considerations.
EXAMPLE: Until the year 2000, the United States controlled the export of cryptographic hardware and
software (considered munitions under United States law). If a U.S. software company worked with a
computer manufacturer in London, the U.S. company could not send cryptographic software to the
manufacturer. The U.S. company first would have to obtain a license to export the software from the
United States. Any security policy that depended on the London manufacturer using that cryptographic
software would need to take this into account.
EXAMPLE: Suppose the law makes it illegal to read a user's file without the user's permission. An
attacker breaks into the system and begins to download users' files. If the system administrators
notice this and observe what the attacker is reading, they will be reading the victim's files without his
permission and therefore will be violating the law themselves. For this reason, most sites require users
to give (implicit or explicit) permission for system administrators to read their files. In some
jurisdictions, an explicit exception allows system administrators to access information on their systems
without permission in order to protect the quality of service provided or to prevent damage to their
systems.
Complicating this issue are situations involving the laws of multiple jurisdictions—especially foreign ones.
EXAMPLE: In the 1990s, the laws involving the use of cryptography in France were very different from
those in the United States. The laws of France required companies sending enciphered data out of the
country to register their cryptographic keys with the government. Security procedures involving the
transmission of enciphered data from a company in the United States to a branch office in France had
to take these differences into account.
EXAMPLE: If a policy called for prosecution of attackers and intruders came from Russia to a system in
the United States, prosecution would involve asking the United States authorities to extradite the
alleged attackers from Russia. This undoubtedly would involve court testimony from company
personnel involved in handling the intrusion, possibly trips to Russia, and more court time once the
extradition was completed. The cost of prosecuting the attackers might be considerably higher than
the company would be willing (or able) to pay.
Laws are not the only constraints on policies and selection of mechanisms. Society distinguishes between legal and
acceptable practices. It may be legal for a company to require all its employees to provide DNA samples for
authentication purposes, but it is not socially acceptable. Requiring the use of social security numbers as
passwords is legal (unless the computer is one owned by the U.S. government) but also unacceptable. These
practices provide security but at an unacceptable cost, and they encourage users to evade or otherwise overcome
the security mechanisms.
The issue that laws and customs raise is the issue of psychological acceptability. A security mechanism that would
put users and administrators at legal risk would place a burden on these people that few would be willing to bear;
thus, such a mechanism would not be used. An unused mechanism is worse than a nonexistent one, because it
gives a false impression that a security service is available. Hence, users may rely on that service to protect their
data, when in reality their data is unprotected.
Top
1.7 Human Issues
Implementing computer security controls is complex, and in a large organization procedural controls often become
vague or cumbersome. Regardless of the strength of the technical controls, if nontechnical considerations affect
their implementation and use, the effect on security can be severe. Moreover, if configured or used incorrectly,
even the best security control is useless at best and dangerous at worst. Thus, the designers, implementers, and
maintainers of security controls are essential to the correct operation of those controls.
1.7.1 Organizational Problems
Security provides no direct financial rewards to the user. It limits losses, but it also requires the expenditure of
resources that could be used elsewhere. Unless losses occur, organizations often believe they are wasting effort
related to security. After a loss, the value of these controls suddenly becomes appreciated. Furthermore, security
controls often add complexity to otherwise simple operations. For example, if concluding a stock trade takes two
minutes without security controls and three minutes with security controls, adding those controls results in a 50%
loss of productivity.
Losses occur when security protections are in place, but such losses are expected to be less than they would have
been without the security mechanisms. The key question is whether such a loss, combined with the resulting loss
in productivity, would be greater than a financial loss or loss of confidence should one of the nonsecured
transactions suffer a breach of security.
Compounding this problem is the question of who is responsible for the security of the company's computers. The
power to implement appropriate controls must reside with those who are responsible; the consequence of not
doing so is that the people who can most clearly see the need for security measures, and who are responsible for
implementing them, will be unable to do so. This is simply sound business practice; responsibility without power
causes problems in any organization, just as does power without responsibility.
Once clear chains of responsibility and power have been established, the need for security can compete on an
equal footing with other needs of the organization. The most common problem a security manager faces is the lack
of people trained in the area of computer security. Another common problem is that knowledgeable people are
overloaded with work. At many organizations, the "security administrator" is also involved in system
administration, development, or some other secondary function. In fact, the security aspect of the job is often
secondary. The problem is that indications of security problems often are not obvious and require time and skill to
spot. Preparation for an attack makes dealing with it less chaotic, but such preparation takes enough time and
requires enough attention so that treating it as a secondary aspect of a job means that it will not be performed
well, with the expected consequences.
Lack of resources is another common problem. Securing a system requires resources as well as people. It requires
time to design a configuration that will provide an adequate level of security, to implement the configuration, and
to administer the system. It requires money to purchase products that are needed to build an adequate security
system or to pay someone else to design and implement security measures. It requires computer resources to
implement and execute the security mechanisms and procedures. It requires training to ensure that employees
understand how to use the security tools, how to interpret the results, and how to implement the nontechnical
aspects of the security policy.
1.7.2 People Problems
The heart of any security system is people. This is particularly true in computer security, which deals mainly with
technological controls that can usually be bypassed by human intervention. For example, a computer system
authenticates a user by asking that user for a secret code; if the correct secret code is supplied, the computer
assumes that the user is authorized to use the system. If an authorized user tells another person his secret code,
the unauthorized user can masquerade as the authorized user with significantly less likelihood of detection.
People who have some motive to attack an organization and are not authorized to use that organization's systems
are called outsiders and can pose a serious threat. Experts agree, however, that a far more dangerous threat
comes from disgruntled employees and other insiders who are authorized to use the computers. Insiders typically
know the organization of the company's systems and what procedures the operators and users follow and often
know enough passwords to bypass many security controls that would detect an attack launched by an outsider.
Insider misuse of authorized privileges is a very difficult problem to solve.
Untrained personnel also pose a threat to system security. As an example, one operator did not realize that the
contents of backup tapes needed to be verified before the tapes were stored. When attackers deleted several
critical system files, she discovered that none of the backup tapes could be read.
System administrators who misread the output of security mechanisms, or do not analyze that output, contribute
to the probability of successful attacks against their systems. Similarly, administrators who misconfigure security-
related features of a system can weaken the site security. Users can also weaken site security by misusing security
mechanisms (such as selecting passwords that are easy to guess).
Lack of training need not be in the technical arena. Many successful break-ins have arisen from the art of social
engineering. If operators will change passwords based on telephone requests, all an attacker needs to do is to
determine the name of someone who uses the computer. A common tactic is to pick someone fairly far above the
operator (such as a vice president of the company) and to feign an emergency (such as calling at night and saying
that a report to the president of the company is due the next morning) so that the operator will be reluctant to
refuse the request. Once the password has been changed to one that the attacker knows, he can simply log in as a
normal user. Social engineering attacks are remarkably successful and often devastating.
The problem of misconfiguration is aggravated by the complexity of many security-related configuration files. For
instance, a typographical error can disable key protection features. Even worse, software does not always work as
advertised. One widely used system had a vulnerability that arose when an administrator made too long a list that
named systems with access to certain files. Because the list was too long, the system simply assumed that the
administrator meant to allow those files to be accessed without restriction on who could access them—exactly the
opposite of what was intended.
Top
1.8 Tying It All Together
The considerations discussed above appear to flow linearly from one to the next (see Figure 1-1). Human issues
pervade each stage of the cycle. In addition, each stage of the cycle feeds back to the preceding stage, and
through that stage to all earlier stages. The operation and maintenance stage is critical to the life cycle. Figure 1-1
breaks it out so as to emphasize the impact it has on all stages. The following example shows the importance of
feedback.
Figure 1-1. The security life cycle.
EXAMPLE: A major corporation decided to improve its security. It hired consultants, determined the
threats, and created a policy. From the policy, the consultants derived several specifications that the
security mechanisms had to meet. They then developed a design that would meet the specifications.
During the implementation phase, the company discovered that employees could connect modems to
the telephones without being detected. The design required all incoming connections to go through a
firewall. The design had to be modified to divide systems into two classes: systems connected to "the
outside," which were put outside the firewall; and all other systems, which were put behind the
firewall. The design needed other modifications as well.
When the system was deployed, the operation and maintenance phase revealed several unexpected
threats. The most serious was that systems were repeatedly misconfigured to allow sensitive data to
be sent across the Internet in the clear. The implementation made use of cryptographic software very
difficult. Once this problem had been remedied, the company discovered that several "trusted" hosts
(those allowed to log in without authentication) were physically outside the control of the company.
This violated policy, but for commercial reasons the company needed to continue to use these hosts.
The policy element that designated these systems as "trusted" was modified. Finally, the company
detected proprietary material being sent to a competitor over electronic mail. This added a threat that
the company had earlier discounted. The company did not realize that it needed to worry about insider
attacks.
Feedback from operation is critical. Whether or not a program is tested or proved to be secure, operational
environments always introduce unexpected problems or difficulties. If the assurance (specification, design,
implementation, and testing/proof) phase is done properly, the extra problems and difficulties are minimal. The
analysts can handle them, usually easily and quickly. If the assurance phase has been omitted or done poorly, the
problems may require a complete reevaluation of the system. The tools used for the feedback include auditing, in
which the operation of the system is recorded and analyzed so that the analyst can determine what the problems
are.
Top
1.9 Summary
Computer security depends on many aspects of a computer system. The threats that a site faces, and the level and
quality of the countermeasures, depend on the quality of the security services and supporting procedures. The
specific mix of these attributes is governed by the site security policy, which is created after careful analysis of the
value of the resources on the system or controlled by the system and of the risks involved.
Underlying all this are key assumptions describing what the site and the system accept as true or trustworthy;
understanding these assumptions is the key to analyzing the strength of the system's security. This notion of
"trust" is the central notion for computer security. If trust is well placed, any system can be made acceptably
secure. If it is misplaced, the system cannot be secure in any sense of the word.
Once this is understood, the reason that people consider security to be a relative attribute is plain. Given enough
resources, an attacker can often evade the security procedures and mechanisms that are in place. Such a desire is
tempered by the cost of the attack, which in some cases can be very expensive. If it is less expensive to
regenerate the data than to launch the attack, most attackers will simply regenerate the data.
This chapter has laid the foundation for what follows. All aspects of computer security begin with the nature of
threats and countering security services. In future chapters, we will build on these basic concepts.
Top
1.10 Research Issues
Future chapters will explore research issues in the technical realm. However, other, nontechnical issues affect the
needs and requirements for technical solutions, and research into these issues helps guide research into technical
areas.
A key question is how to quantify risk. The research issue is how to determine the effects of a system's
vulnerabilities on its security. For example, if a system can be compromised in any of 50 ways, how can a company
compare the costs of the procedures (technical and otherwise) needed to prevent the compromises with the costs
of detecting the compromises, countering them, and recovering from them? Many methods assign weights to the
various factors, but these methods are ad hoc. A rigorous technique for determining appropriate weights has yet to
be found.
The relationships of computer security to the political, social, and economic aspects of the world are not well
understood. How does the ubiquity of the Internet change a country's borders? If someone starts at a computer in
France, transits networks that cross Switzerland, Germany, Poland, Norway, Sweden, and Finland, and launches an
attack on a computer in Russia, who has jurisdiction? How can a country limit the economic damage caused by an
attack on its computer networks? How can attacks be traced to their human origins?
This chapter has also raised many technical questions. Research issues arising from them will be explored in future
chapters.
Top
1.11 Further Reading
Risk analysis arises in a variety of contexts. Molak [725] presents essays on risk management and analysis in a
variety of fields. Laudan [610] provides an enjoyable introduction to the subject. Neumann [772] discusses the
risks of technology and recent problems. Software safety (Leveson [622]) requires an understanding of the risks
posed in the environment. Peterson [804] discusses many programming errors in a readable way. All provide
insights into the problems that arise in a variety of environments.
Many authors recount stories of security incidents. The earliest, Parker's wonderful book [799], discusses motives
and personalities as well as technical details. Stoll recounts the technical details of uncovering an espionage ring
that began as the result of a 75¢ accounting error [973, 975]. Hafner and Markoff describe the same episode in a
study of "cyberpunks" [432]. The Internet worm [322, 432, 845, 953] brought the problem of computer security
into popular view. Numerous other incidents [374, 432, 642, 914, 931, 968] have heightened public awareness of
the problem.
Several books [59, 61, 824, 891] discuss computer security for the layperson. These works tend to focus on
attacks that are visible or affect the end user (such as pornography, theft of credit card information, and
deception). They are worth reading for those who wish to understand the results of failures in computer security.
Top
1.12 Exercises
1: Classify each of the following as a violation of confidentiality, of integrity, of availability, or of some
combination thereof.
a. John copies Mary's homework.
b. Paul crashes Linda's system.
c. Carol changes the amount of Angelo's check from $100 to $1,000.
d. Gina forges Roger's signature on a deed.
e. Rhonda registers the domain name "AddisonWesley.com" and refuses to let the publishing
house buy or use that domain name.
f. Jonah obtains Peter's credit card number and has the credit card company cancel the card and
replace it with another card bearing a different account number.
g. Henry spoofs Julie's IP address to gain access to her computer.
2: Identify mechanisms for implementing the following. State what policy or policies they might be
enforcing.
a. A password changing program will reject passwords that are less than five characters long or
that are found in the dictionary.
b. Only students in a computer science class will be given accounts on the department's
computer system.
c. The login program will disallow logins of any students who enter their passwords incorrectly
three times.
d. The permissions of the file containing Carol's homework will prevent Robert from cheating and
copying it.
e. When World Wide Web traffic climbs to more than 80% of the network's capacity, systems will
disallow any further communications to or from Web servers.
f. Annie, a systems analyst, will be able to detect a student using a program to scan her system
for vulnerabilities.
g. A program used to submit homework will turn itself off just after the due date.
3: The aphorism "security through obscurity" suggests that hiding information provides some level of
security. Give an example of a situation in which hiding information does not add appreciably to the
security of a system. Then give an example of a situation in which it does.
4: Give an example of a situation in which a compromise of confidentiality leads to a compromise in
integrity.
5: Show that the three security services—confidentiality, integrity, and availability—are sufficient to deal
with the threats of disclosure, disruption, deception, and usurpation.
6: In addition to mathematical and informal statements of policy, policies can be implicit (not stated).
Why might this be done? Might it occur with informally stated policies? What problems can this cause?
7: For each of the following statements, give an example of a situation in which the statement is true.
a. Prevention is more important than detection and recovery.
b. Detection is more important than prevention and recovery.
c. Recovery is more important than prevention and detection.
8: Is it possible to design and implement a system in which no assumptions about trust are made? Why
or why not?
9: Policy restricts the use of electronic mail on a particular system to faculty and staff. Students cannot
send or receive electronic mail on that host. Classify the following mechanisms as secure, precise, or
broad.
a. The electronic mail sending and receiving programs are disabled.
b. As each letter is sent or received, the system looks up the sender (or recipient) in a database.
If that party is listed as faculty or staff, the mail is processed. Otherwise, it is rejected.
(Assume that the database entries are correct.)
c. The electronic mail sending programs ask the user if he or she is a student. If so, the mail is
refused. The electronic mail receiving programs are disabled.
10: Consider a very high-assurance system developed for the military. The system has a set of
specifications, and both the design and implementation have been proven to satisfy the specifications.
What questions should school administrators ask when deciding whether to purchase such a system
for their school's use?
11: How do laws protecting privacy impact the ability of system administrators to monitor user activity?
12: Computer viruses are programs that, among other actions, can delete files without a user's
permission. A U.S. legislator wrote a law banning the deletion of any files from computer disks. What
was the problem with this law from a computer security point of view? Specifically, state which
security service would have been affected if the law had been passed.
13: Users often bring in programs or download programs from the Internet. Give an example of a site for
which the benefits of allowing users to do this outweigh the dangers. Then give an example of a site
for which the dangers of allowing users to do this outweigh the benefits.
14: A respected computer scientist has said that no computer can ever be made perfectly secure. Why
might she have said this?
15: An organization makes each lead system administrator responsible for the security of the system he
or she runs. However, the management determines what programs are to be on the system and how
they are to be configured.
a. Describe the security problem(s) that this division of power would create.
b. How would you fix them?
16: The president of a large software development company has become concerned about competitors
learning proprietary information. He is determined to stop them. Part of his security mechanism is to
require all employees to report any contact with employees of the company's competitors, even if it is
purely social. Do you believe this will have the desired effect? Why or why not?
17: The police and the public defender share a computer. What security problems does this present? Do
you feel it is a reasonable cost-saving measure to have all public agencies share the same (set of)
computers?
18: Companies usually restrict the use of electronic mail to company business but do allow minimal use
for personal reasons.
a. How might a company detect excessive personal use of electronic mail, other than by reading
it? (Hint: Think about the personal use of a company telephone.)
b. Intuitively, it seems reasonable to ban all personal use of electronic mail on company
computers. Explain why most companies do not do this.
19: Argue for or against the following proposition. Ciphers that the government cannot cryptanalyze
should be outlawed. How would your argument change if such ciphers could be used provided that the
users registered the keys with the government?
20: For many years, industries and financial institutions hired people who broke into their systems once
those people were released from prison. Now, such a conviction tends to prevent such people from
being hired. Why you think attitudes on this issue changed? Do you think they changed for the better
or for the worse?
21: A graduate student accidentally releases a program that spreads from computer system to computer
system. It deletes no files but requires much time to implement the necessary defenses. The graduate
student is convicted. Despite demands that he be sent to prison for the maximum time possible (to
make an example of him), the judge sentences him to pay a fine and perform community service.
What factors do you believe caused the judge to hand down the sentence he did? What would you
have done were you the judge, and what extra information would you have needed to make your
decision?
Top
